{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 4\n",
    "\n",
    "**Due: Feb 5 @ 11:59pm Pacific Time on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Please solve questions 1 and 2, and choose one of questions 3 or 4.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "\n",
    "-Handi Zhao(hdzhao@stanford.edu);\n",
    "\n",
    "-Sylvia Sun(ys3835@stanford.edu);\n",
    "\n",
    "-Zhengji Yang(yangzj@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from rl.approximate_dynamic_programming import (\n",
    "    value_iteration,\n",
    "    approximate_policy_iteration\n",
    ")\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, StateActionMapping\n",
    "from numpy import allclose\n",
    "from rl.distribution import Choose\n",
    "import numpy as np\n",
    "from typing import Mapping, Iterable, Tuple, Dict\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess, StateActionMapping\n",
    "\n",
    "from dataclasses import dataclass, field, replace\n",
    "from rl.dynamic_programming import policy_iteration, value_iteration_result\n",
    "from rl.function_approx import FunctionApprox, Tabular\n",
    "from dataclasses import dataclass\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Implement *Approximate Policy Iteration*, generalization of the tabular\n",
    "Policy Iteration we covered in the previous class. In order to implement\n",
    "Approximate Policy Iteration, first review the interface and\n",
    "implementation of *Approximate Policy Evaluation* and *Approximate Value\n",
    "Iteration* (in file\n",
    "[rl/approximate_dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/approximate_dynamic_programming.py)),\n",
    "then design the interface of *Approximate Policy Iteration* to be the\n",
    "same as that of *Approximate Value Iteration*. Note that your\n",
    "implementation of *Approximate Policy Iteration* would need to invoke\n",
    "*Approximate Policy Evaluation* since Policy Evaluation is a component\n",
    "of Policy Iteration. Test that your implementation is correct in two\n",
    "ways:\n",
    "\n",
    "-   Ensure that *Approximate Policy Iteration* gives the same Optimal\n",
    "    Value Function/Optimal Policy as that obtained by *Approximate Value\n",
    "    Iteration*.\n",
    "\n",
    "-   Ensure that *Approximate Policy Iteration* produces the same result\n",
    "    as our prior implementation of Policy Iteration (in file\n",
    "    [rl/dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py)).\n",
    "    For this you need to pass to your implementation of *Approximate\n",
    "    Policy Iteration* a `FiniteMarkovDecisionProcess` input and a\n",
    "    `Tabular` instance for the `FunctionApprox` input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approximate_policy_iteration(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    gamma: float,\n",
    "    approx_0: ValueFunctionApprox[S],\n",
    "    non_terminal_states_distribution: NTStateDistribution[S],\n",
    "    num_state_samples: int,\n",
    "    num_iterations: int\n",
    ") -> Iterator[Tuple[ValueFunctionApprox[S], DeterministicPolicy[S, A]]]:\n",
    "    \"\"\"\n",
    "    Approximate Policy Iteration function.\n",
    "\n",
    "    Parameters:\n",
    "    mdp: MarkovDecisionProcess[S, A] - The given Markov Decision Process.\n",
    "    gamma: float - Discount factor.\n",
    "    approx_0: ValueFunctionApprox[S] - Initial value function approximation.\n",
    "    non_terminal_states_distribution: NTStateDistribution[S] - Distribution of non-terminal states.\n",
    "    num_state_samples: int - Number of state samples for approximation calculation.\n",
    "    num_iterations: int - Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "    Iterator[Tuple[ValueFunctionApprox[S], DeterministicPolicy[S, A]]] - Iterator of value function approximations and policies.\n",
    "    \"\"\"\n",
    "    value_function = approx_0\n",
    "    policy = DeterministicPolicy(lambda s: np.random.choice(mdp.actions(NonTerminal(s))))\n",
    "\n",
    "    for _ in range(num_iterations):\n",
    "        # Approximate Policy Evaluation\n",
    "        value_function = evaluate_mrp(\n",
    "            mdp.apply_policy(policy),\n",
    "            gamma,\n",
    "            value_function,\n",
    "            non_terminal_states_distribution,\n",
    "            num_state_samples\n",
    "        ).__next__()\n",
    "\n",
    "        # Policy Improvement\n",
    "        def action_for_state(s: NonTerminal[S]) -> A:\n",
    "            return max(\n",
    "                ((mdp.step(s, a).expectation(lambda s_r: s_r[1] + gamma * extended_vf(value_function, s_r[0])), a)\n",
    "                 for a in mdp.actions(s)),\n",
    "                key=itemgetter(0)\n",
    "            )[1]\n",
    "\n",
    "        policy = DeterministicPolicy(action_for_state)\n",
    "\n",
    "        yield (value_function, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test1: Verify that Approximate Policy Iteration gives results as that obtained by Approximate Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Value Functions: True\n"
     ]
    }
   ],
   "source": [
    "# design a simple mdp for testing\n",
    "class SimpleGamblingMDP(FiniteMarkovDecisionProcess[int, int]):\n",
    "    def __init__(self, max_capital: int, win_prob: float):\n",
    "        \"\"\"\n",
    "        Initialize the Simple Gambling MDP.\n",
    "\n",
    "        Parameters:\n",
    "        max_capital (int): The maximum capital.\n",
    "        win_prob (float): Probability of winning a bet.\n",
    "        \"\"\"\n",
    "        self.max_capital = max_capital\n",
    "        self.win_prob = win_prob\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> StateActionMapping[int, int]:\n",
    "        \"\"\"\n",
    "        Get the action transition reward map for the MDP.\n",
    "\n",
    "        Returns:\n",
    "        StateActionMapping[int, int]: The action transition reward mapping.\n",
    "        \"\"\"\n",
    "        d: Dict[int, Dict[int, Categorical[Tuple[int, float]]]] = {}\n",
    "        for capital in range(1, self.max_capital):\n",
    "            action_dict: Dict[int, Categorical[Tuple[int, float]]] = {}\n",
    "            for bet in range(1, min(capital, self.max_capital - capital) + 1):\n",
    "                outcomes = {\n",
    "                    (capital + bet, bet): self.win_prob,  # Winning outcome\n",
    "                    (capital - bet, -bet): 1 - self.win_prob  # Losing outcome\n",
    "                }\n",
    "                action_dict[bet] = Categorical(outcomes)\n",
    "            d[capital] = action_dict\n",
    "        return d\n",
    "\n",
    "\n",
    "max_capital = 100\n",
    "win_prob = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "# Creating an instance of MDP\n",
    "gambling_mdp = SimpleGamblingMDP(max_capital=max_capital, win_prob=win_prob)\n",
    "\n",
    "initial_values = {s: 0.0 for s in range(1, max_capital)}\n",
    "approx_0 = Tabular(values_map=initial_values, count_to_weight_func=lambda n: 1.0 / n)\n",
    "\n",
    "# Approximate value iteration\n",
    "value_function_approx = value_iteration(\n",
    "    mdp=gambling_mdp,\n",
    "    Î³=gamma,\n",
    "    approx_0=approx_0,\n",
    "    non_terminal_states_distribution=Choose(range(1, max_capital)),\n",
    "    num_state_samples=1000\n",
    ").__next__()\n",
    "\n",
    "# Approximate policy iteration\n",
    "policy_iteration_result = approximate_policy_iteration(\n",
    "    mdp=gambling_mdp,\n",
    "    gamma=gamma,\n",
    "    approx_0=approx_0,\n",
    "    non_terminal_states_distribution=Choose(range(1, max_capital)),\n",
    "    num_state_samples=1000,\n",
    "    num_iterations=100\n",
    ").__next__()\n",
    "\n",
    "# Check if value functions from both methods are close enough\n",
    "tolerance = 1e-5\n",
    "similar = all(\n",
    "    abs(value_function_approx.values_map[s] - policy_iteration_result[0].values_map.get(s, 0)) <= tolerance\n",
    "    for s in value_function_approx.values_map\n",
    ")\n",
    "print(f\"Similar Value Functions: {similar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test2: Ensure that Approximate Policy Iteration produces the same results as our prior implementation of Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the value functions close enough (tolerance 0.001)? Yes\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "max_capital = 100\n",
    "win_prob = 0.5\n",
    "gamma = 1.0\n",
    "tolerance = 0.001\n",
    "\n",
    "gambling_mdp = SimpleGamblingMDP(max_capital=max_capital, win_prob=win_prob)\n",
    "\n",
    "# Use policy_iteration function\n",
    "policy_iter_result = policy_iteration(gambling_mdp, gamma)\n",
    "\n",
    "# Retrieve the final value function and policy\n",
    "optimal_value_function, optimal_policy = next(islice(policy_iter_result, 100, None))\n",
    "\n",
    "# Retrieve the final results from approximate policy iteration\n",
    "approximate_value_function = policy_iteration_result[0].values_map\n",
    "\n",
    "#function to test whether two output are close\n",
    "def within_tolerance(v1: Dict[int, float], v2: Dict[int, float], tolerance: float) -> bool:\n",
    "    return all(abs(v1[s] - v2.get(s, 0.0)) <= tolerance for s in v1)\n",
    "# Check if the optimal and approximate value functions are close enough\n",
    "are_close = within_tolerance(optimal_value_function, approximate_value_function, tolerance)\n",
    "print(f\"Are the value functions close enough (tolerance {tolerance})? {'Yes' if are_close else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Assume the Utility function is $U(x) = x - \\frac {\\alpha x^2} 2$.\n",
    "Assuming $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$, calculate:\n",
    "\n",
    "-   Expected Utility $\\mathbb{E}[U(x)]$\n",
    "\n",
    "-   Certainty-Equivalent Value $x_{CE}$\n",
    "\n",
    "-   Absolute Risk-Premium $\\pi_A$\n",
    "\n",
    "\n",
    "Assume you have a million dollars to invest for a year and you are\n",
    "allowed to invest $z$ dollars in a risky asset whose annual return on\n",
    "investment is $\\mathcal{N}(\\mu, \\sigma^2)$ and the remaining (a million\n",
    "minus $z$ dollars) would need to be invested in a riskless asset with\n",
    "fixed annual return on investment of $r$. You are not allowed to adjust\n",
    "the quantities invested in the risky and riskless assets after your\n",
    "initial investment decision at time $t=0$ (static asset allocation\n",
    "problem). If your risk-aversion is based on this Utility function, how\n",
    "much would you invest in the risky asset? In other words, what is the\n",
    "optimal value for $z$, given your level of risk-aversion (determined by\n",
    "a fixed value of $\\alpha$)?\n",
    "\n",
    "Plot how the optimal value of $z$ varies with $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1). Expected Utility: \n",
    " \n",
    " \\begin{align}\n",
    " \\mathbb{E}[U(x)] & = \\mathbb{E}[x] - \\dfrac{\\alpha}{2} \\mathbb{E}[x^2] \\\\\n",
    "                  & = \\mathbb{E}[x] - \\dfrac{\\alpha}{2} (\\text{Var}[x] + \\mathbb{E}[x]^2)\\\\\n",
    "                  & = \\mu - \\dfrac{\\alpha}{2}(\\sigma^2 + \\mu^2) \n",
    " \\end{align}\n",
    "\n",
    " 2). Certainty-Equivalent Value:\n",
    "\n",
    " \\begin{align}\n",
    " x_{CE} &= U^{-1}(\\mathbb{E}[U(x)]) \\\\\n",
    "        &= \\dfrac{1 \\pm \\sqrt{1 - 2\\alpha\\mathbb{E}[U(x)]}}{\\alpha} \\\\\n",
    "        &= \\dfrac{1 \\pm \\sqrt{\\alpha^2\\sigma^2 + (\\alpha\\mu - 1)^2}}{\\alpha}\n",
    " \\end{align}\n",
    "\n",
    " 3). Absolute Risk-Premium:\n",
    "\n",
    " \\begin{align}\n",
    "\\pi_A &= \\mathbb{E}[x] - x_{CE}\\\\\n",
    "      &= \\mu - \\dfrac{1 \\pm \\sqrt{\\alpha^2\\sigma^2 + (\\alpha\\mu - 1)^2}}{\\alpha}\\\\\n",
    "      &= \\dfrac{\\alpha\\mu - 1 \\pm \\sqrt{\\alpha^2\\sigma^2 + (\\alpha\\mu - 1)^2}}{\\alpha}\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the situation of investing $z$ dollars in risky asset, $1M - z$ dollars in rates, the synthetic portfolio has a return of $$X_z \\sim \\mathcal{N}(z\\mu + (1M - z)r, z^2\\sigma^2).$$\n",
    "\n",
    "From the expression of expected utility of a Gaussian R.V., we get\n",
    "\\begin{align}\n",
    "\\mathbb{E}[U(X_z)] &= z\\mu + (1M - z)r - \\dfrac{\\alpha}{2}(z^2\\sigma^2 + (z\\mu + (1M - z)r)^2)\\\\\n",
    "                 &= (\\mu - r)z + 1M \\cdot r - \\dfrac{\\alpha}{2}[\\sigma^2 + (\\mu-r)^2]z^2 - \\dfrac{\\alpha}{2}2(1M\\cdot r) \\cdot (\\mu - r)z + \\dfrac{\\alpha}{2}(1M\\cdot r)^2\\\\\n",
    "                 &= -\\dfrac{\\alpha[\\sigma^2 + (\\mu-r)^2]}{2}z^2 + [1 - \\alpha(1M\\cdot r)](\\mu - r)z + 1M \\cdot r \n",
    "                 + \\dfrac{\\alpha}{2}(1M\\cdot r)^2\\\\\n",
    "\\end{align}\n",
    "\n",
    "Thus, if the risk-aversion is perfectly measured by certainty equivalent value $x_{CE}$ with parameter $\\alpha$, the optimal value is given by\n",
    "\\begin{align}\n",
    "z^* &= \\text{argmax}_z\\,\\, U^{-1}(\\mathbb{E}[U(X_z)])\\\\\n",
    "    &= \\text{argmax}_z\\,\\, \\dfrac{1 \\pm \\sqrt{\\alpha^2\\sigma^2 z^2 + (\\alpha\\cdot 1M\\cdot r + \\alpha (\\mu - r) z - 1)^2}}{\\alpha}\\\\\n",
    "    &= \\text{argmax}_{z}\\,\\, \\alpha^2[\\sigma^2 + (\\mu-r)^2] z^2 - 2\\alpha[1 - \\alpha(1M\\cdot r)](\\mu - r)z + [1 - \\alpha(1M\\cdot r)]^2\\\\\n",
    "    &= \\left\\{\n",
    "        \\begin{aligned}\n",
    "        &0,\\quad  & & \\dfrac{[1 - \\alpha(1M\\cdot r)](\\mu - r)}{\\alpha[\\sigma^2 + (\\mu-r)^2]} \\geq 0.5M\\\\\n",
    "        &1M,\\quad & & o.w.\n",
    "        \\end{aligned}\n",
    "        \\right.\\\\\n",
    "    &= \\left\\{\n",
    "        \\begin{aligned}\n",
    "        &0,\\quad  & & \\alpha \\geq \\dfrac{2(\\mu - r)*10^{-6}}{\\sigma^2 + \\mu^2 - r^2}\\\\\n",
    "        &1M,\\quad & & o.w.\n",
    "        \\end{aligned}\n",
    "        \\right.\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'optimal value for z')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu = 0.1\n",
    "r = 0.05\n",
    "sigma = 0.15\n",
    "alpha0 = 0.4/(1e6*r)\n",
    "alpha = np.linspace(alpha0/100, alpha0, 10000)\n",
    "z = ((1 - alpha*1e6*r)*(mu - r)/(alpha * (sigma**2 + (mu-r)**2)) < 5e5)\n",
    "plt.plot(alpha,z)\n",
    "plt.xlabel('risk aversion parameter alpha')\n",
    "plt.ylabel('optimal value for z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Assume you are playing a casino game where at every turn, if you bet a\n",
    "quantity $x$, you will be returned $x \\cdot (1 + \\alpha)$ with\n",
    "probability $p$ and returned $x \\cdot (1 - \\beta)$ with probability\n",
    "$q = 1 - p$ for $\\alpha, \\beta \\in \\mathbb{R}^+$ (i.e., the return on\n",
    "bet is $\\alpha$ with probability $p$ and $-\\beta$ with probability\n",
    "$q = 1-p$) . The problem is to identify a betting strategy that will\n",
    "maximize one's expected wealth over the long run. The optimal solution\n",
    "to this problem is known as the Kelly criterion, which involves betting\n",
    "a constant fraction of one's wealth at each turn (let us denote this\n",
    "optimal fraction as $f^*$).\n",
    "\n",
    "It is known that the Kelly criterion (formula for $f^*$) is equivalent\n",
    "to maximizing the Expected Utility of Wealth after a single bet, with\n",
    "the Utility function defined as: $U(W) = \\log(W)$. Denote your wealth\n",
    "before placing the single bet as $W_0$. Let $f$ be the fraction (to be\n",
    "solved for) of $W_0$ that you will bet. Therefore, your bet is\n",
    "$f \\cdot W_0$.\n",
    "\n",
    "-   Write down the two outcomes for wealth $W$ at the end of your single\n",
    "    bet of $f \\cdot W_0$.\n",
    "\n",
    "-   Write down the two outcomes for $\\log$ (Utility) of $W$.\n",
    "\n",
    "-   Write down $\\mathbb{E}[\\log(W)]$.\n",
    "\n",
    "-   Take the derivative of $\\mathbb{E}[\\log(W)]$ with respect to $f$.\n",
    "\n",
    "-   Set this derivative to 0 to solve for $f^*$. Verify that this is\n",
    "    indeed a maxima by evaluating the second derivative at $f^*$. This\n",
    "    formula for $f^*$ is known as the Kelly Criterion.\n",
    "\n",
    "-   Convince yourself that this formula for $f^*$ makes intuitive sense\n",
    "    (in terms of it's dependency on $\\alpha$, $\\beta$ and $p$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Derive the solution to Merton's Portfolio problem for the case of the\n",
    "$\\log(\\cdot)$ Utility function. Note that the derivation in the textbook\n",
    "is for CRRA Utility function with $\\gamma \\neq 1$ and the case of the\n",
    "$\\log(\\cdot)$ Utility function was left as an exercise to the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the fraction of wealth allocated to the risky asset at time $t$ as $ \\pi (t, W_t)$ and the fraction of wealth allocated to the riskless asset at time $t$ is $1 - \\pi (t, W_t)$. The infinitesimal change in wealth $dW_t$ from time $t$ to time $t + dt$ is given by: $$dW_t = ((r+\\pi_t \\cdot (\\mu - r))Â·W_t - c_t) \\cdot dt+\\pi_t \\cdot \\sigma \\cdot W_t \\cdot dz_t$$\n",
    "    The goal is to determine optimal $\\pi(t, W_t), c(t, W_t)$ at any time $t$ to maximize: $$\\mathbb{E}[\\int_t^T e^{- \\rho(s-t)} \\cdot \\log(c_s) \\cdot ds + e^{- \\rho(T-t)} \\cdot B(T) \\cdot \\log(W_T) | W_t]$$ where $B(T)$ is the bequest and take $B(T) = 1/\\rho$ for simplicity.\n",
    "    The Hamilton-Jacobi-Bellman equation is then $$\\max_{\\pi_t,c_t} \\{ \\mathbb{E}_t [dV^*(t, W_t)] + \\log(c_t) \\cdot dt\\} = \\rho \\cdot V^*(t, W_t) \\cdot dt$$ \n",
    "    Then, use Itoâs Lemma on $dV^*$, remove the $dz_t$ term, and divide throughout by $dt$ to get $$\\max_{\\pi_t,c_t} \\{ \\frac{\\partial V^*}{\\partial t}+\\frac{\\partial V^*}{\\partial W_t} \\cdot ((r+\\pi_t \\cdot (\\mu - r))Â·W_t - c_t) + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\cdot \\frac{\\pi_t^2 \\cdot \\sigma^2 \\cdot W_t^2}{2} + \\log(c_t)\\} = \\rho \\cdot V^*(t, W_t)$$ with the terminal condition $$V^*(T, W_T) = \\frac{1}{\\rho}\\log(W_T)$$\n",
    "    Take partial derivative with respect to $\\pi_t^*$, $c_t^*$ and equate to 0 to get $$(\\mu-r) \\cdot \\frac{\\partial V^*}{\\partial W_t} + \\frac{\\partial^2 V^*}{\\partial W_t^2} \\cdot \\pi_t \\cdot \\sigma^2 \\cdot W_t = 0 \\implies \\pi_t^*=\\frac{-\\frac{\\partial V^*}{\\partial W_t} \\cdot (\\mu-r)}{\\frac{\\partial^2 V^*}{\\partial W_t^2} \\cdot \\sigma^2 \\cdot W_t}$$ \n",
    "    $$-\\frac{\\partial V^*}{\\partial W_t} + \\frac{1}{c_t^*} = 0 \\implies c_t^* = \\frac{\\partial W_t}{\\partial V^*}$$\n",
    "    Substitute $\\pi_t^*$ and $c_t^*$ back to get the Optimal Value Function Partial Differential Equation: $$\\frac{\\partial V^*}{\\partial t}-\\frac{(\\mu-r)^2}{2\\sigma^2} \\cdot \\frac{(\\frac{\\partial V^*}{\\partial W_t})^2}{\\frac{\\partial^2 V^*}{\\partial W_t^2}} + \\frac{\\partial V^*}{\\partial W_t} \\cdot r \\cdot W_t + \\log(\\frac{\\partial W_t}{\\partial V^*}) - 1 = \\rho \\cdot V^*(t, W_t)$$\n",
    "    We surmise with a guess solution in terms of a deterministic function (f) of time: $$V^*(t, W_t) = f(t) + \\frac{1}{\\rho}log(W_t)$$ then $$\\frac{\\partial V^*}{\\partial t} = f'(t)$$ $$\\frac{\\partial V^*}{\\partial W_t} = \\frac{1}{\\rho W_t}$$ $$\\frac{\\partial^2 V^*}{\\partial W_t^2} = -\\frac{1}{\\rho W_t^2}$$\n",
    "    In this case, $\\displaystyle \\pi_t^* (t, W_t)=\\frac{\\mu-r}{\\sigma^2}$ and $c_t^* (t, W_t) = \\rho W_t$ .\n",
    "    Substituting the guess solution in the PDE, we get the simple ODE: $f'(t) - \\rho f(t) + \\nu = 0$ where $\\displaystyle \\nu=\\frac{(\\mu-r)^2}{2\\rho \\sigma^2}+\\frac{r-\\rho}{\\rho} + log(\\rho)$ with boundary condition $f(T)=0$. The solution of the ODE is then $$f(t)=\\frac{\\nu}{\\rho} (1-e^{-\\rho(T-t)})$$\n",
    "    Substituting $f(t)$ into $V^*(t, W_t)$ to get $$V^*(t, W_t)=\\frac{\\nu}{\\rho} (1-e^{-\\rho(T-t)})+\\frac{1}{\\rho}log(W_t)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
