{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 5\n",
    "\n",
    "**Due: Feb 12 @ 11:59pm Pacific Time on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Solve any 3 of the 4 questions.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/my-username/my-repo/assignment-file-name.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "\n",
    "-Handi Zhao(hdzhao@stanford.edu);\n",
    "\n",
    "-Sylvia Sun(ys3835@stanford.edu);\n",
    "\n",
    "-Zhengji Yang(yangzj@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/TikhonJelvis/RL-book.git\n",
    "import os\n",
    "import sys\n",
    "os.chdir('RL-book/rl')\n",
    "sys.path.append('/content/RL-book')\n",
    "import numpy as np\n",
    "import itertools\n",
    "from typing import Callable, Tuple, Iterable\n",
    "from rl.distribution import Gaussian, Choose, SampledDistribution\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, State, TransitionStep\n",
    "from rl.function_approx import LinearFunctionApprox, FunctionApprox\n",
    "from rl.approximate_dynamic_programming import value_iteration\n",
    "from rl.policy import Policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "You are a milkvendor and your task is to bring to your store a supply\n",
    "(denoted $S \\in \\mathbb{R}$) of milk volume in the morning that will\n",
    "give you the best profits. You know that the demand for milk through the\n",
    "course of the day is a probability distribution function $f$ (for\n",
    "mathematical convenience, assume people can buy milk in volumes that are\n",
    "real numbers, hence milk demand $x \\in \\mathbb{R}$ is a continuous\n",
    "variable with a probability density function). For every extra gallon of\n",
    "milk you carry at the end of the day (supply $S$ exceeds random demand\n",
    "$x$), you incur a cost of $h$ (effectively the wasteful purchases\n",
    "amounting to the difference between your purchase price and end-of-day\n",
    "discount disposal price since you are not allowed to sell the same milk\n",
    "the next day). For every gallon of milk that a customer demands that you\n",
    "don't carry (random demand $x$ exceeds supply $S$), you incur a cost of\n",
    "$p$ (effectively the missed sales revenue amounting to the difference\n",
    "between your sales price and purchase price). So your task is to\n",
    "identify the optimal supply $S$ that minimizes your Expected Cost\n",
    "$g(S)$, given by the following:\n",
    "\n",
    "$$g_1(S) = E[\\max(x-S, 0)] = \\int_{-\\infty}^{\\infty} \\max(x-S, 0) \\cdot f(x) \\cdot dx = \\int_S^{\\infty} (x-S) \\cdot f(x) \\cdot dx$$\n",
    "$$g_2(S) = E[\\max(S-x, 0)] = \\int_{-\\infty}^{\\infty} \\max(S-x, 0) \\cdot f(x) \\cdot dx = \\int_{-\\infty}^S (S-x) \\cdot f(x) \\cdot dx$$\n",
    "\n",
    "$$g(S) = p \\cdot g_1(S) + h \\cdot g_2(S)$$\n",
    "\n",
    "After you solve this problem, see if you can frame this problem in terms\n",
    "of a call/put options portfolio problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 Answer:\n",
    "\n",
    "By definition, optimal supply $S^*$ is defined by\n",
    "\\begin{align}\n",
    "S^* &= \\text{argmin}_{S}\\,\\, g(S)\\\\\n",
    "    &= \\text{argmin}_{S}\\,\\, p \\cdot g_1(S) + h \\cdot g_2(S)\n",
    "\\end{align}\n",
    "\n",
    "As it's clear that both $g_1$ and $g_2$ are differentiable w.r.t. $S$, thus, $S^*$ satisfies the following first-order condition:\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial g(S^*)}{\\partial S} &= \\dfrac{\\partial [p \\cdot g_1(S^*) + h \\cdot g_2(S^*)]}{\\partial S} \\\\\n",
    "                    &= p \\cdot \\int_{S^*}^\\infty \\dfrac{\\partial (x - S^*)\\cdot f(x)}{\\partial S} dx + \n",
    "  h \\cdot \\int_{-\\infty}^{S^*} \\dfrac{\\partial (S^* - x)\\cdot f(x)}{\\partial S} dx \\\\\n",
    "&= - p \\cdot \\int_{S^*}^\\infty f(x) dx + h \\cdot \\int_{-\\infty}^{S^*} f(x) dx\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "\n",
    "Let $F(x) = \\int^x_{-\\infty} f(s) ds$ be the cumulative distribution function of x, then the above formula can be rewritten as\n",
    "\n",
    "\\begin{equation}\n",
    "- p (1 - F(S^*)) + h F(S^*) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "(p + h) \\cdot F(S^*) = p\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Thus, optimal supply $S^*$ is\n",
    "$$ S^* = F^{-1}(\\dfrac{p}{h + p}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call/Put Option Portfolio Problem:\n",
    "\n",
    "For an underlying X with pdf $f$, the premiums for the European call and put option on $X$ are denoted by $p$ and $h$, i.e. $p+1$ and $h+1$ signify the ratios of the option price over its value. Now, if a customer aims to hedge against the volatility of X by constructing a portfolio comprising long 1 share of the call option and long 1 share of the put option, what is the optimal strike price $S$ such that the overall premium is minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "[rl/chapter8/optimal_bin_tree.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter8/optimal_exercise_bin_tree.py)\n",
    "models the American Payoff pricing problem as a `FiniteMarkovDecisionProcess` in the form of a binary tree with only two\n",
    "discrete transitions for any given asset price. In the world of mathematical and computational finance, it is common practice to work\n",
    "with continuous-valued asset prices and transitions to a continuous set of asset prices for the next time step. Your task is to model this\n",
    "problem as a `MarkovDecisionProcess` (not finite) with discrete time, continuous-valued asset prices and a continuous-set of transitions.\n",
    "Assume an arbitrary probability distribution for asset price movements from one time step to another, so you'd be sampling from the arbitrary\n",
    "transition probability distribution. Hence, you will be solving this problem with Approximate Dynamic Programming using the code in\n",
    "[rl/approximate_dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/approximate_dynamic_programming.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated value at price 100: 0.20144928564100928\n"
     ]
    }
   ],
   "source": [
    "class ContinuousAssetPriceState(State):\n",
    "    def __init__(self, price: float):\n",
    "        self.price = price\n",
    "\n",
    "class TradingAction(float):\n",
    "    pass\n",
    "\n",
    "class ContinuousAssetTradingMDP(MarkovDecisionProcess[ContinuousAssetPriceState, TradingAction]):\n",
    "    def __init__(self, rate: float, vol: float, payoff: Callable[[float], float], action_range: Tuple[float, float]):\n",
    "        self.rate = rate\n",
    "        self.vol = vol\n",
    "        self.payoff = payoff\n",
    "        self.action_range = action_range\n",
    "\n",
    "    def step(self, state: ContinuousAssetPriceState, action: TradingAction) -> SampledDistribution[Tuple[ContinuousAssetPriceState, float]]:\n",
    "        # Transition model for MDP\n",
    "        def next_state_reward_sampler():\n",
    "            next_price_mean = state.price * np.exp(self.rate - 0.5 * self.vol ** 2)\n",
    "            next_price_std = state.price * self.vol\n",
    "            next_price = np.random.normal(next_price_mean, next_price_std)\n",
    "            action_value = max(min(float(action), self.action_range[1]), self.action_range[0])\n",
    "            reward = self.payoff(next_price) * action_value\n",
    "            next_state = ContinuousAssetPriceState(price=next_price)\n",
    "            return next_state, reward\n",
    "        return SampledDistribution(next_state_reward_sampler)\n",
    "\n",
    "    def actions(self, state: ContinuousAssetPriceState) -> Iterable[TradingAction]:\n",
    "        num_actions = 100\n",
    "        action_step = (self.action_range[1] - self.action_range[0]) / num_actions\n",
    "        return (TradingAction(a) for a in np.arange(self.action_range[0], self.action_range[1], action_step))\n",
    "\n",
    "def create_linear_function_approximator() -> FunctionApprox[ContinuousAssetPriceState]:\n",
    "    # Linear function approximator\n",
    "    return LinearFunctionApprox.create(feature_functions=[lambda s: s.price, lambda s: 1.0])\n",
    "\n",
    "def non_terminal_states_distribution(mean: float, std_dev: float, num_samples: int) -> Choose[ContinuousAssetPriceState]:\n",
    "    samples = np.random.normal(mean, std_dev, num_samples)\n",
    "    return Choose([ContinuousAssetPriceState(price) for price in samples])\n",
    "\n",
    "\n",
    "rate = 0.05\n",
    "vol = 0.2\n",
    "strike = 100\n",
    "payoff = lambda price: max(price - strike, 0)\n",
    "mdp = ContinuousAssetTradingMDP(rate, vol, payoff, action_range=(0, 1e4))\n",
    "\n",
    "function_approximator = create_linear_function_approximator()\n",
    "\n",
    "mean_price = 100\n",
    "std_dev_price = 10\n",
    "num_state_samples = 20\n",
    "\n",
    "gamma = 0.95\n",
    "num_iterations = 10\n",
    "\n",
    "# Value iteration process\n",
    "optimal_value_function_iterator = value_iteration(\n",
    "    mdp=mdp,\n",
    "    γ=gamma,\n",
    "    approx_0=function_approximator,\n",
    "    non_terminal_states_distribution=non_terminal_states_distribution(mean_price, std_dev_price, num_state_samples),\n",
    "    num_state_samples=num_state_samples\n",
    ")\n",
    "\n",
    "# Optimal value function retrieval\n",
    "optimal_value_function = next(itertools.islice(optimal_value_function_iterator, num_iterations, None))\n",
    "\n",
    "# Result display\n",
    "current_price = 100\n",
    "state = ContinuousAssetPriceState(current_price)\n",
    "estimated_value = optimal_value_function(state)\n",
    "print(f\"Estimated value at price {current_price}: {estimated_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "We'd like to build a simple simulator of Order Book Dynamics as a\n",
    "`MarkovProcess` using the code in\n",
    "[rl/chapter9/order_book.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter9/order_book.py).\n",
    "An object of type `OrderBook` constitutes the *State*. Your task is to\n",
    "come up with a simple model for random arrivals of Market Orders and\n",
    "Limit Orders based on the current contents of the `OrderBook`. This\n",
    "model of random arrivals of Marker Orders and Limit Orders defines the\n",
    "probabilistic transitions from the current state (`OrderBook` object) to\n",
    "the next state (`OrderBook` object). Implement the probabilistic\n",
    "transitions as a `MarkovProcess` and use it's `simulate` method to\n",
    "complete your implementation of a simple simulator of Order Book\n",
    "Dynamics.\n",
    "\n",
    "Experiment with different models for random arrivals of Market Orders\n",
    "and Limit Orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Derive the expressions for the Optimal Value Function and Optimal Policy\n",
    "for the *Linear-Percentage Temporary* (LPT) Price Impact Model\n",
    "formulated by Bertsimas and Lo. The LPT model is described below for all\n",
    "$t = 0, 1, \\ldots T-1$:\n",
    "\n",
    "$$P_{t+1} = P_t \\cdot e^{Z_t}$$ \n",
    "\n",
    "$$X_{t+1} = \\rho \\cdot X_t + \\eta_t$$\n",
    "\n",
    "$$Q_t = P_t \\cdot (1 - \\beta \\cdot N_t - \\theta \\cdot X_t)$$ \n",
    "\n",
    "where $Z_t$ are independent and identically distributed random variables with mean\n",
    "$\\mu_Z$ and variance $\\sigma^2_Z$ for all $t = 0, 1, \\ldots, T-1$,\n",
    "$\\eta_t$ are independent and identically distributed random variables\n",
    "with mean 0 for all $t = 0, 1, \\ldots, T-1$, $Z_t$ and $\\eta_t$ are\n",
    "independent of each other for all $t = 0, 1, \\ldots, T-1$, and\n",
    "$\\rho, \\beta, \\theta$ are given constants. The model assumes no\n",
    "risk-aversion (Utility function is the identity function) and so, the\n",
    "objective is to maximize the Expected Total Sales Proceeds over the\n",
    "finite-horizon up to time $T$ (discount factor is 1). In your\n",
    "derivation, use the same methodology as we followed for the *Simple\n",
    "Linear Price Impact Model with no Risk-Aversion*.\n",
    "\n",
    "Implement this LPT model by customizing the class\n",
    "`OptimalOrderExecution` in [rl/chapter9/optimal_order_execution.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter9/optimal_order_execution.py).\n",
    "\n",
    "Compare the obtained Optimal Value Function and Optimal Policy against\n",
    "the closed-form solution you derived above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 Answer:\n",
    "Using the definition in the linear impact with no risk-aversion model, we have states $s_t := (P_t, R_t, X_t) \\in \\mathcal{S}_t$ and actions $a_t := N_t \\in \\mathcal{A}_t$. Similarly, we also have $R_t=N-\\sum_{i=0}^{t-1}N_i$, $R_0=N$. $\\forall \\, t<T$, $R_{t+1}=R_t-N_t$ and $N_{T-1}=R_{T-1}$. Sales proceeds in time step $t$ is $N_t \\cdot Q_t=N_t\\cdot P_t(1-\\beta N_t-\\theta X_t))$. We need to find the policy $\\pi_t^*((P_t, R_t))=N_t^*$ that maximize $$\\mathbb{E} \\left[ \\sum_{t=0}^{T-1} U(N_t, Q_t) \\right]$$\n",
    "    Value function for policy $\\pi$ is $$V_t^{\\pi}((P_t, R_t))=\\mathbb{E}_{\\pi} \\left[ \\sum_{i=t}^T N_i \\cdot P_i(1-\\beta N_i -\\theta X_i)\\right]$$ and the optimal value function is $\\displaystyle V_t^*((P_t, R_t))= \\max_{\\pi} V_t^{\\pi}((P_t, R_t))$. The optimal value function satisfy the Bellman Equation $$V_t^*((P_t, R_t))=\\max_{N_t} \\{N_t \\cdot P_t(1-\\beta N_t -\\theta X_t)+\\mathbb{E}[V_{t+1}^*((P_{t+1}, R_{t+1}))]\\}$$\n",
    "In this case, \n",
    "\\begin{align*}\n",
    "    V_{T-1}^*((P_{T-1}, R_{T-1})) &= N_{T-1} \\cdot P_{T-1}(1-\\beta N_{T-1} -\\theta X_{T-1}) \\\\\n",
    "    &=R_{T-1} \\cdot P_{T-1}(1-\\beta R_{T-1} -\\theta X_{T-1})\n",
    "\\end{align*}\n",
    "Then,\n",
    "\\begin{align*}\n",
    "    V_{T-2}^*((P_{T-2}, R_{T-2})) =& \\max_{N_{T-2}} \\{N_{T-2} \\cdot P_{T-2}(1-\\beta N_{T-2} -\\theta X_{T-2}) + \\mathbb{E}[R_{T-1} \\cdot P_{T-1}(1-\\beta R_{T-1} -\\theta X_{T-1})]\\} \\\\\n",
    "    =& \\max_{N_{T-2}} \\{N_{T-2} \\cdot P_{T-2}(1-\\beta N_{T-2} -\\theta X_{T-2}) + \\mathbb{E}[(R_{T-2}-N_{T-2}) \\cdot P_{T-2}e^{Z_{T-2}}(1-\\beta (R_{T-2}-N_{T-2}) -\\theta (\\rho X_{T-2}+\\eta_{T-2})]\\} \\\\\n",
    "    =& \\max_{N_{T-2}} \\{N_{T-2} \\cdot P_{T-2}(1-\\beta N_{T-2} -\\theta X_{T-2}) + (R_{T-2}-N_{T-2})P_{T-2}e^{\\mu_Z+\\sigma^2_Z/2} -\\beta (R_{T-2}-N_{T-2})^2P_{T-2}e^{\\mu_Z+\\sigma^2_Z/2} \\\\\n",
    "    &-\\theta \\rho X_{T-2}(R_{T-2}-N_{T-2})P_{T-2}e^{\\mu_Z+\\sigma^2_Z/2}\\} \\\\\n",
    "\\end{align*}\n",
    "Taking partial derivatives w.r.t. $N_{T-2}$, we have $$P_{T-2}(1-2\\beta N_{T-2}-\\theta X_{T-2}-e^{\\mu_Z+\\sigma^2_Z/2}+2\\beta e^{\\mu_Z+\\sigma^2_Z/2} R_{T-2}-2\\beta e^{\\mu_Z+\\sigma^2_Z/2}N_{T-2}+\\theta \\rho X_{T-2}e^{\\mu_Z+\\sigma^2_Z/2})=0$$ so that $$N_{T-2}=\\frac{\\theta(\\rho e^{\\mu_Z+\\sigma^2_Z/2}-1)}{2\\beta(e^{\\mu_Z+\\sigma^2_Z/2}+1)}X_{T-2}+\\frac{e^{\\mu_Z+\\sigma^2_Z/2}}{e^{\\mu_Z+\\sigma^2_Z/2}+1}R_{T-2}+\\frac{1-e^{\\mu_Z+\\sigma^2_Z/2}}{2\\beta(1+e^{\\mu_Z+\\sigma^2_Z/2})}$$ and $$V_{T-2}^*((P_{T-2}, R_{T-2})) = e^{\\mu_Z+\\sigma^2_Z/2} P_{T-2} (c^{(4)}_{T-2} + c^{(5)}_{T-2} R_{T-2} + c^{(6)}_{T-2} X_{T-2} + c^{(7)}_{T-2} R_{T-2}^2 + c^{(8)}_{T-2} X_{T-2}^2 + c^{(9)}_{T-2} R_{T-2} X_{T-2})$$\n",
    "Continuing backwards in time in this manner gives: \n",
    "$$N_t^* = c^{(1)}_t + c^{(2)}_t R_t + c^{(3)}_t X_t $$\n",
    "\n",
    "\\begin{align*}\n",
    "    V^*_t((P_t,R_t,X_t)) = e^{\\mu_Z + \\frac {\\sigma_Z^2} 2} \\cdot P_t \\cdot ( & c^{(4)}_t + c^{(5)}_t R_t + c^{(6)}_t X_t + c^{(7)}_t R_t^2 + c^{(8)}_t X_t^2 + c^{(9)}_t R_t X_t)\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Callable, Sequence, Tuple, Iterator\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose\n",
    "from rl.function_approx import FunctionApprox, LinearFunctionApprox\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, NonTerminal, State\n",
    "from rl.policy import DeterministicPolicy\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy, ValueFunctionApprox\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction: VF And Policy\n",
      "---------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Optimal Sales = 2, Opt Val = 10059.739\n",
      "\n",
      "Optimal Weights below:\n",
      "[1.00485747 0.00111643]\n",
      "\n",
      "Time 1\n",
      "\n",
      "Optimal Sales = 7, Opt Val = 10053.597\n",
      "\n",
      "Optimal Weights below:\n",
      "[1.00413703 0.00122269]\n",
      "\n",
      "Time 2\n",
      "\n",
      "Optimal Sales = 0, Opt Val = 10044.374\n",
      "\n",
      "Optimal Weights below:\n",
      "[1.00332429 0.00111313]\n",
      "\n",
      "Time 3\n",
      "\n",
      "Optimal Sales = 0, Opt Val = 10031.879\n",
      "\n",
      "Optimal Weights below:\n",
      "[1.00197266 0.00121522]\n",
      "\n",
      "Time 4\n",
      "\n",
      "Optimal Sales = 1, Opt Val = 10012.952\n",
      "\n",
      "Optimal Weights below:\n",
      "[0.99960358 0.00169166]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class PriceSharesAndMarket:\n",
    "    price: float\n",
    "    market_factor: float\n",
    "    shares: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OptimalOrderExecution:\n",
    "    shares: int\n",
    "    time_steps: int\n",
    "    avg_exec_price_diff: Sequence[Callable[[PriceSharesAndMarket], float]]\n",
    "    price_dynamics: Sequence[Callable[[PriceSharesAndMarket], Distribution[float]]]\n",
    "    market_dynamics: Sequence[Callable[[PriceSharesAndMarket], Distribution[float]]]\n",
    "    utility_func: Callable[[float], float]\n",
    "    discount_factor: float\n",
    "    func_approx: ValueFunctionApprox[PriceSharesAndMarket]\n",
    "    initial_price_distribution: Distribution[float]\n",
    "    init_market_factor: float\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[PriceSharesAndMarket, int]:\n",
    "        \"\"\"\n",
    "        State is (Price P_t, Remaining Shares R_t)\n",
    "        Action is shares sold N_t\n",
    "        \"\"\"\n",
    "\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "        price_diff: Sequence[Callable[[PriceSharesAndMarket], float]] = self.avg_exec_price_diff\n",
    "        price_dynamics: Sequence[Callable[[PriceSharesAndMarket], Distribution[float]]] = self.price_dynamics\n",
    "        market_dynamics: Sequence[Callable[[PriceSharesAndMarket], Distribution[float]]] = self.market_dynamics\n",
    "        steps: int = self.time_steps\n",
    "\n",
    "        class OptimalExecutionMDP(MarkovDecisionProcess[PriceSharesAndMarket, int]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                p_r: NonTerminal[PriceSharesAndMarket],\n",
    "                sell: int\n",
    "            ) -> SampledDistribution[Tuple[State[PriceSharesAndMarket],\n",
    "                                           float]]:\n",
    "\n",
    "                def sr_sampler_func(\n",
    "                    p_r=p_r,\n",
    "                    sell=sell\n",
    "                ) -> Tuple[State[PriceSharesAndMarket], float]:\n",
    "                    p_s: PriceSharesAndMarket = PriceSharesAndMarket(\n",
    "                        price=p_r.state.price,\n",
    "                        market_factor=p_r.state.market_factor,\n",
    "                        shares=sell\n",
    "                    )\n",
    "                    next_price: float = price_dynamics[t](p_s).sample()\n",
    "                    next_market: float = market_dynamics[t](p_s).sample()\n",
    "                    next_rem: int = p_r.state.shares - sell\n",
    "                    next_state: PriceSharesAndMarket = PriceSharesAndMarket(\n",
    "                        price=next_price,\n",
    "                        market_factor=next_market,\n",
    "                        shares=next_rem\n",
    "                    )\n",
    "                    reward: float = utility_f(\n",
    "                        sell * p_r.state.price * (1 - price_diff[t](p_s))\n",
    "                    )\n",
    "                    return (NonTerminal(next_state), reward)\n",
    "\n",
    "                return SampledDistribution(\n",
    "                    sampler=sr_sampler_func,\n",
    "                    expectation_samples=100\n",
    "                )\n",
    "\n",
    "            def actions(self, p_s: NonTerminal[PriceSharesAndMarket]) -> \\\n",
    "                    Iterator[int]:\n",
    "                if t == steps - 1:\n",
    "                    return iter([p_s.state.shares])\n",
    "                else:\n",
    "                    return iter(range(p_s.state.shares + 1))\n",
    "\n",
    "        return OptimalExecutionMDP()\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> \\\n",
    "            SampledDistribution[NonTerminal[PriceSharesAndMarket]]:\n",
    "\n",
    "        def states_sampler_func() -> NonTerminal[PriceSharesAndMarket]:\n",
    "            price: float = self.initial_price_distribution.sample()\n",
    "            market_factor = self.init_market_factor\n",
    "            rem: int = self.shares\n",
    "            for i in range(t):\n",
    "                sell: int = Choose(range(rem + 1)).sample()\n",
    "                price = self.price_dynamics[i](PriceSharesAndMarket(\n",
    "                    price=price,\n",
    "                    market_factor=market_factor,\n",
    "                    shares=rem\n",
    "                )).sample()\n",
    "                market_factor = self.market_dynamics[i](PriceSharesAndMarket(\n",
    "                    price=price,\n",
    "                    market_factor=market_factor,\n",
    "                    shares=rem\n",
    "                )).sample()\n",
    "                rem -= sell\n",
    "            return NonTerminal(PriceSharesAndMarket(\n",
    "                price=price,\n",
    "                market_factor=market_factor,\n",
    "                shares=rem\n",
    "            ))\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self\n",
    "    ) -> Iterator[Tuple[ValueFunctionApprox[PriceSharesAndMarket],\n",
    "                        DeterministicPolicy[PriceSharesAndMarket, int]]]:\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[PriceSharesAndMarket, int],\n",
    "            ValueFunctionApprox[PriceSharesAndMarket],\n",
    "            SampledDistribution[NonTerminal[PriceSharesAndMarket]]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            self.func_approx,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps)]\n",
    "\n",
    "        num_state_samples: int = 10000\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=self.discount_factor,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    from rl.distribution import Gaussian\n",
    "\n",
    "    init_price_mean: float = 100.0\n",
    "    init_price_stdev: float = 10.0\n",
    "    init_market_factor = 0\n",
    "    num_shares: int = 100\n",
    "    num_time_steps: int = 5\n",
    "    beta: float = 5e-7\n",
    "    theta: float = 0.005\n",
    "    rho = 0.5\n",
    "    μ_z: float = 0\n",
    "    σ_z: float = 0.02/np.sqrt(13)\n",
    "    σ_eta: float = np.sqrt(1 - rho**2)\n",
    "\n",
    "    price_diff = [lambda p_s: beta * p_s.shares + theta * p_s.market_factor for _ in range(num_time_steps)]\n",
    "    market_dynamics = [lambda p_s: Gaussian(μ=rho * p_s.market_factor, σ=σ_eta) for _ in range(num_time_steps)]\n",
    "    price_dynamics = [lambda p_s: Gaussian(μ=μ_z, σ=σ_z).map(lambda a: np.exp(a) * p_s.price) for _ in range(num_time_steps)]\n",
    "    ffs = [\n",
    "        lambda p_s: p_s.state.price * p_s.state.shares,\n",
    "        lambda p_s: float(p_s.state.shares * p_s.state.shares)\n",
    "    ]\n",
    "    fa: FunctionApprox = LinearFunctionApprox.create(feature_functions=ffs)\n",
    "    init_price_distrib: Gaussian = Gaussian(\n",
    "        μ=init_price_mean,\n",
    "        σ=init_price_stdev\n",
    "    )\n",
    "\n",
    "    ooe: OptimalOrderExecution = OptimalOrderExecution(\n",
    "        shares=num_shares,\n",
    "        time_steps=num_time_steps,\n",
    "        avg_exec_price_diff=price_diff,\n",
    "        price_dynamics=price_dynamics,\n",
    "        market_dynamics=market_dynamics,\n",
    "        utility_func=lambda x: x,\n",
    "        discount_factor=1,\n",
    "        func_approx=fa,\n",
    "        initial_price_distribution=init_price_distrib,\n",
    "        init_market_factor=init_market_factor,\n",
    "    )\n",
    "    it_vf: Iterator[Tuple[ValueFunctionApprox[PriceSharesAndMarket],\n",
    "                          DeterministicPolicy[PriceSharesAndMarket, int]]] = ooe.backward_induction_vf_and_pi()\n",
    "\n",
    "    state: PriceSharesAndMarket = PriceSharesAndMarket(\n",
    "        price=init_price_mean,\n",
    "        market_factor=init_market_factor,\n",
    "        shares=num_shares\n",
    "    )\n",
    "    print(\"Backward Induction: VF And Policy\")\n",
    "    print(\"---------------------------------\")\n",
    "    print()\n",
    "    for t, (vf, pol) in enumerate(it_vf):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        opt_sale: int = pol.action_for(state)\n",
    "        val: float = vf(NonTerminal(state))\n",
    "        print(f\"Optimal Sales = {opt_sale:d}, Opt Val = {val:.3f}\")\n",
    "        print()\n",
    "        print(\"Optimal Weights below:\")\n",
    "        print(vf.weights.weights)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
